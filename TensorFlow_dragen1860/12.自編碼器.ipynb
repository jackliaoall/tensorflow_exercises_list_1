{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"12.自編碼器.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPDtFUzsZrSQ9EvgHY1lLWz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3mE_TXPNMIqm"},"source":["**自编码器实战**"]},{"cell_type":"code","metadata":{"id":"T0ff4rt9MMJ2"},"source":["import  os\n","import  tensorflow as tf\n","import  numpy as np\n","from    tensorflow import keras\n","from    tensorflow.keras import Sequential, layers\n","from    PIL import Image\n","from    matplotlib import pyplot as plt\n","\n","tf.random.set_seed(22)\n","np.random.seed(22)\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","assert tf.__version__.startswith('2.')\n","\n","def save_images(imgs, name):\n","    new_im = Image.new('L', (280, 280))\n","\n","    index = 0\n","    for i in range(0, 280, 28):\n","        for j in range(0, 280, 28):\n","            im = imgs[index]\n","            im = Image.fromarray(im, mode='L')\n","            new_im.paste(im, (i, j))\n","            index += 1\n","\n","    new_im.save(name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_HQcDBFtLbmw"},"source":["h_dim = 20\n","batchsz = 512\n","lr = 1e-3\n","\n","(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n","x_train, x_test = x_train.astype(np.float32) / 255., x_test.astype(np.float32) / 255.\n","# we do not need label\n","train_db = tf.data.Dataset.from_tensor_slices(x_train)\n","train_db = train_db.shuffle(batchsz * 5).batch(batchsz)\n","test_db = tf.data.Dataset.from_tensor_slices(x_test)\n","test_db = test_db.batch(batchsz)\n","\n","print(x_train.shape, y_train.shape)\n","print(x_test.shape, y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Sx_QTV0Lqv_"},"source":["class AE(keras.Model):\n","\n","    def __init__(self):\n","        super(AE, self).__init__()\n","\n","        # Encoders\n","        self.encoder = Sequential([\n","            layers.Dense(256, activation=tf.nn.relu),\n","            layers.Dense(128, activation=tf.nn.relu),\n","            layers.Dense(h_dim)\n","        ])\n","\n","        # Decoders\n","        self.decoder = Sequential([\n","            layers.Dense(128, activation=tf.nn.relu),\n","            layers.Dense(256, activation=tf.nn.relu),\n","            layers.Dense(784)\n","        ])\n","\n","\n","    def call(self, inputs, training=None):\n","        # [b, 784] => [b, 10]\n","        h = self.encoder(inputs)\n","        # [b, 10] => [b, 784]\n","        x_hat = self.decoder(h)\n","\n","        return x_hat\n","\n","model = AE()\n","model.build(input_shape=(None, 784))\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M5zt8pevL2h2"},"source":["optimizer = tf.optimizers.Adam(lr=lr)\n","\n","for epoch in range(100):\n","\n","    for step, x in enumerate(train_db):\n","\n","        #[b, 28, 28] => [b, 784]\n","        x = tf.reshape(x, [-1, 784])\n","\n","        with tf.GradientTape() as tape:\n","            x_rec_logits = model(x)\n","\n","            rec_loss = tf.losses.binary_crossentropy(x, x_rec_logits, from_logits=True)\n","            rec_loss = tf.reduce_mean(rec_loss)\n","\n","        grads = tape.gradient(rec_loss, model.trainable_variables)\n","        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","\n","        if step % 100 ==0:\n","            print(epoch, step, float(rec_loss))\n","\n","\n","        # evaluation\n","        x = next(iter(test_db))\n","        logits = model(tf.reshape(x, [-1, 784]))\n","        x_hat = tf.sigmoid(logits)\n","        # [b, 784] => [b, 28, 28]\n","        x_hat = tf.reshape(x_hat, [-1, 28, 28])\n","\n","        # [b, 28, 28] => [2b, 28, 28]\n","        x_concat = tf.concat([x, x_hat], axis=0)\n","        x_concat = x_hat\n","        x_concat = x_concat.numpy() * 255.\n","        x_concat = x_concat.astype(np.uint8)\n","        save_images(x_concat, 'ae_images/rec_epoch_%d.png'%epoch)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KROw1fqQL7Ev"},"source":["**变分自动编码器实战**"]},{"cell_type":"code","metadata":{"id":"Ym_kcg5-L46e"},"source":["z_dim = 10\n","\n","class VAE(keras.Model):\n","\n","    def __init__(self):\n","        super(VAE, self).__init__()\n","\n","        # Encoder\n","        self.fc1 = layers.Dense(128)\n","        self.fc2 = layers.Dense(z_dim) # get mean prediction\n","        self.fc3 = layers.Dense(z_dim)\n","\n","        # Decoder\n","        self.fc4 = layers.Dense(128)\n","        self.fc5 = layers.Dense(784)\n","\n","    def encoder(self, x):\n","\n","        h = tf.nn.relu(self.fc1(x))\n","        # get mean\n","        mu = self.fc2(h)\n","        # get variance\n","        log_var = self.fc3(h)\n","\n","        return mu, log_var\n","\n","    def decoder(self, z):\n","\n","        out = tf.nn.relu(self.fc4(z))\n","        out = self.fc5(out)\n","\n","        return out\n","\n","    def reparameterize(self, mu, log_var):\n","\n","        eps = tf.random.normal(log_var.shape)\n","\n","        std = tf.exp(log_var*0.5)\n","\n","        z = mu + std * eps\n","        return z\n","\n","    def call(self, inputs, training=None):\n","\n","        # [b, 784] => [b, z_dim], [b, z_dim]\n","        mu, log_var = self.encoder(inputs)\n","        # reparameterization trick\n","        z = self.reparameterize(mu, log_var)\n","\n","        x_hat = self.decoder(z)\n","\n","        return x_hat, mu, log_var"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xFvR1G6kMBm2"},"source":["model = VAE()\n","model.build(input_shape=(4, 784))\n","optimizer = tf.optimizers.Adam(lr)\n","\n","for epoch in range(1000):\n","\n","    for step, x in enumerate(train_db):\n","\n","        x = tf.reshape(x, [-1, 784])\n","\n","        with tf.GradientTape() as tape:\n","            x_rec_logits, mu, log_var = model(x)\n","\n","            rec_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=x_rec_logits)\n","            rec_loss = tf.reduce_sum(rec_loss) / x.shape[0]\n","\n","            # compute kl divergence (mu, var) ~ N (0, 1)\n","            # https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians\n","            kl_div = -0.5 * (log_var + 1 - mu**2 - tf.exp(log_var))\n","            kl_div = tf.reduce_sum(kl_div) / x.shape[0]\n","\n","            loss = rec_loss + 1. * kl_div\n","\n","        grads = tape.gradient(loss, model.trainable_variables)\n","        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","\n","        if step % 100 == 0:\n","            print(epoch, step, 'kl div:', float(kl_div), 'rec loss:', float(rec_loss))\n","\n","\n","    # evaluation\n","    z = tf.random.normal((batchsz, z_dim))\n","    logits = model.decoder(z)\n","    x_hat = tf.sigmoid(logits)\n","    x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() *255.\n","    x_hat = x_hat.astype(np.uint8)\n","    save_images(x_hat, 'vae_images/sampled_epoch%d.png'%epoch)\n","\n","    x = next(iter(test_db))\n","    x = tf.reshape(x, [-1, 784])\n","    x_hat_logits, _, _ = model(x)\n","    x_hat = tf.sigmoid(x_hat_logits)\n","    x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() *255.\n","    x_hat = x_hat.astype(np.uint8)\n","    save_images(x_hat, 'vae_images/rec_epoch%d.png'%epoch)"],"execution_count":null,"outputs":[]}]}