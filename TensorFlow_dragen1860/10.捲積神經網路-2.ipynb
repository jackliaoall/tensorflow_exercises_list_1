{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10.捲積神經網路-2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOp8k6lQyQVXyOO/9wq+gni"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e91b14d1da8247629960f625778e19d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_10268640fb71456792fbebc34284d81e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e8f630c481e64d2f8214d1e4cebb6dc7","IPY_MODEL_3d6e8a960f0b4193b62a0519c1b36ae3","IPY_MODEL_ec7314092db640ffa39287f69a96d3f1"]}},"10268640fb71456792fbebc34284d81e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e8f630c481e64d2f8214d1e4cebb6dc7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3e3d75d9fa6c4d7aa5ce43c70506244c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 30%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b5201da60cdc452aafbc0deddbbaa3d7"}},"3d6e8a960f0b4193b62a0519c1b36ae3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_525b7d61e09f4badb28b1821fdd186e3","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":100,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":30,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ad51a95685c0460cbe478eb9579ecd0d"}},"ec7314092db640ffa39287f69a96d3f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_07392641d73444eeb8678702109bfbfb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 30/100 [58:40&lt;2:16:21, 116.88s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7331c611ab8647afa6588edd4934a327"}},"3e3d75d9fa6c4d7aa5ce43c70506244c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b5201da60cdc452aafbc0deddbbaa3d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"525b7d61e09f4badb28b1821fdd186e3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ad51a95685c0460cbe478eb9579ecd0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"07392641d73444eeb8678702109bfbfb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7331c611ab8647afa6588edd4934a327":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"NY9IQWqz_SJz"},"source":["**ResnetNet**"]},{"cell_type":"code","metadata":{"id":"TgR2y03G_Szq"},"source":["import  tensorflow as tf\n","from    tensorflow import keras\n","from    tensorflow.keras import layers, Sequential\n","from tqdm import tqdm_notebook\n","\n","import  tensorflow as tf\n","from    tensorflow.keras import layers, optimizers, datasets, Sequential\n","import  os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n","tf.random.set_seed(2345)\n","\n","def preprocess(x, y):\n","    # 将数据映射到-1~1\n","    x = 2*tf.cast(x, dtype=tf.float32) / 255. - 1\n","    y = tf.cast(y, dtype=tf.int32) # 类型转换\n","    return x,y\n","\n","class BasicBlock(layers.Layer):\n","    # 残差模块\n","    def __init__(self, filter_num, stride=1):\n","        super(BasicBlock, self).__init__()\n","        # 第一个卷积单元\n","        self.conv1 = layers.Conv2D(filter_num, (3, 3), strides=stride, padding='same')\n","        self.bn1 = layers.BatchNormalization()\n","        self.relu = layers.Activation('relu')\n","        # 第二个卷积单元\n","        self.conv2 = layers.Conv2D(filter_num, (3, 3), strides=1, padding='same')\n","        self.bn2 = layers.BatchNormalization()\n","\n","        if stride != 1:# 通过1x1卷积完成shape匹配\n","            self.downsample = Sequential()\n","            self.downsample.add(layers.Conv2D(filter_num, (1, 1), strides=stride))\n","        else:# shape匹配，直接短接\n","            self.downsample = lambda x:x\n","\n","    def call(self, inputs, training=None):\n","\n","        # [b, h, w, c]，通过第一个卷积单元\n","        out = self.conv1(inputs)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        # 通过第二个卷积单元\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        # 通过identity模块\n","        identity = self.downsample(inputs)\n","        # 2条路径输出直接相加\n","        output = layers.add([out, identity])\n","        output = tf.nn.relu(output) # 激活函数\n","\n","        return output\n","\n","class ResNet(keras.Model):\n","    # 通用的ResNet实现类\n","    def __init__(self, layer_dims, num_classes=10): # [2, 2, 2, 2]\n","        super(ResNet, self).__init__()\n","        # 根网络，预处理\n","        self.stem = Sequential([layers.Conv2D(64, (3, 3), strides=(1, 1)),\n","                    layers.BatchNormalization(),\n","                    layers.Activation('relu'),\n","                    layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')\n","                    ])\n","        # 堆叠4个Block，每个block包含了多个BasicBlock,设置步长不一样\n","        self.layer1 = self.build_resblock(64,  layer_dims[0])\n","        self.layer2 = self.build_resblock(128, layer_dims[1], stride=2)\n","        self.layer3 = self.build_resblock(256, layer_dims[2], stride=2)\n","        self.layer4 = self.build_resblock(512, layer_dims[3], stride=2)\n","\n","        # 通过Pooling层将高宽降低为1x1\n","        self.avgpool = layers.GlobalAveragePooling2D()\n","        # 最后连接一个全连接层分类\n","        self.fc = layers.Dense(num_classes)\n","\n","    def call(self, inputs, training=None):\n","        # 通过根网络\n","        x = self.stem(inputs)\n","        # 一次通过4个模块\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        # 通过池化层\n","        x = self.avgpool(x)\n","        # 通过全连接层\n","        x = self.fc(x)\n","\n","        return x\n","\n","    def build_resblock(self, filter_num, blocks, stride=1):\n","        # 辅助函数，堆叠filter_num个BasicBlock\n","        res_blocks = Sequential()\n","        # 只有第一个BasicBlock的步长可能不为1，实现下采样\n","        res_blocks.add(BasicBlock(filter_num, stride))\n","\n","        for _ in range(1, blocks):#其他BasicBlock步长都为1\n","            res_blocks.add(BasicBlock(filter_num, stride=1))\n","\n","        return res_blocks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GgKRdJbd_k7D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634567050019,"user_tz":-480,"elapsed":15363,"user":{"displayName":"Liao Jack","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16157886839679822522"}},"outputId":"f890c951-58ba-4625-ffaf-f00b436ed79c"},"source":["(x,y), (x_test, y_test) = datasets.cifar10.load_data() # 加载数据集\n","y = tf.squeeze(y, axis=1) # 删除不必要的维度\n","y_test = tf.squeeze(y_test, axis=1) # 删除不必要的维度\n","print(x.shape, y.shape, x_test.shape, y_test.shape)\n","\n","train_db = tf.data.Dataset.from_tensor_slices((x,y)) # 构建训练集\n","# 随机打散，预处理，批量化\n","train_db = train_db.shuffle(1000).map(preprocess).batch(512)\n","\n","test_db = tf.data.Dataset.from_tensor_slices((x_test,y_test)) #构建测试集\n","# 随机打散，预处理，批量化\n","test_db = test_db.map(preprocess).batch(512)\n","# 采样一个样本\n","sample = next(iter(train_db))\n","print('sample:', sample[0].shape, sample[1].shape,\n","      tf.reduce_min(sample[0]), tf.reduce_max(sample[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 6s 0us/step\n","170508288/170498071 [==============================] - 6s 0us/step\n","(50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,)\n","sample: (512, 32, 32, 3) (512,) tf.Tensor(-1.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)\n"]}]},{"cell_type":"code","metadata":{"id":"O9vK_Ywz_nDr","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["e91b14d1da8247629960f625778e19d5","10268640fb71456792fbebc34284d81e","e8f630c481e64d2f8214d1e4cebb6dc7","3d6e8a960f0b4193b62a0519c1b36ae3","ec7314092db640ffa39287f69a96d3f1","3e3d75d9fa6c4d7aa5ce43c70506244c","b5201da60cdc452aafbc0deddbbaa3d7","525b7d61e09f4badb28b1821fdd186e3","ad51a95685c0460cbe478eb9579ecd0d","07392641d73444eeb8678702109bfbfb","7331c611ab8647afa6588edd4934a327"]},"outputId":"d4f7e7f6-6d76-45d4-89e8-ad91863ff444"},"source":["train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","test_loss = tf.keras.metrics.Mean(name='test_loss')\n","\n","# [b, 32, 32, 3] => [b, 1, 1, 512]\n","model = ResNet([2, 2, 2, 2]) # ResNet18网络\n","# model = ResNet([3, 4, 6, 3]) # ResNet34网络\n","model.build(input_shape=(None, 32, 32, 3))\n","model.summary() # 统计网络参数\n","optimizer = optimizers.Adam(lr=1e-4) # 构建优化器\n","\n","for epoch in tqdm_notebook(range(100)): # 训练epoch\n","    for step, (x,y) in enumerate(train_db):\n","        with tf.GradientTape() as tape:\n","            # [b, 32, 32, 3] => [b, 10],前向传播\n","            logits = model(x)\n","            train_accuracy(y,logits)\n","            # [b] => [b, 10],one-hot编码\n","            y_onehot = tf.one_hot(y, depth=10)\n","            # 计算交叉熵\n","            loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n","            loss = tf.reduce_mean(loss)\n","        # 计算梯度信息\n","        grads = tape.gradient(loss, model.trainable_variables)\n","        # 更新网络参数\n","        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","        if step %50 == 0:\n","            print(epoch, step, 'train loss:', float(loss),'train acc: {:.4f}'.format(train_accuracy.result()*100))\n","            train_accuracy.reset_states()\n","\n","\n","    total_num = 0\n","    total_correct = 0\n","    for x,y in test_db:\n","\n","        logits = model(x)\n","        loss = tf.losses.sparse_categorical_crossentropy(y, logits, from_logits=True)\n","        test_loss(loss)\n","        \n","        prob = tf.nn.softmax(logits, axis=1)\n","        pred = tf.argmax(prob, axis=1)\n","        pred = tf.cast(pred, dtype=tf.int32)\n","        \n","        correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n","        correct = tf.reduce_sum(correct)\n","\n","        total_num += x.shape[0]\n","        total_correct += int(correct)\n","\n","    acc = total_correct / total_num\n","    print(epoch,'test loss:{:.4f}'.format(test_loss.result()), 'test acc:', acc)\n","    test_loss.reset_states()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"res_net\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","sequential (Sequential)      (None, 30, 30, 64)        2048      \n","_________________________________________________________________\n","sequential_1 (Sequential)    (None, 30, 30, 64)        148736    \n","_________________________________________________________________\n","sequential_2 (Sequential)    (None, 15, 15, 128)       526976    \n","_________________________________________________________________\n","sequential_4 (Sequential)    (None, 8, 8, 256)         2102528   \n","_________________________________________________________________\n","sequential_6 (Sequential)    (None, 4, 4, 512)         8399360   \n","_________________________________________________________________\n","global_average_pooling2d (Gl multiple                  0         \n","_________________________________________________________________\n","dense (Dense)                multiple                  5130      \n","=================================================================\n","Total params: 11,184,778\n","Trainable params: 11,176,970\n","Non-trainable params: 7,808\n","_________________________________________________________________\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e91b14d1da8247629960f625778e19d5","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["0 0 train loss: 2.304671287536621 train acc: 12.1094\n","0 50 train loss: 1.7234169244766235 train acc: 27.0273\n","0 test loss:1.5824 test acc: 0.41\n","1 0 train loss: 1.5249770879745483 train acc: 37.5533\n","1 50 train loss: 1.4251024723052979 train acc: 42.7812\n","1 test loss:1.4177 test acc: 0.48\n","2 0 train loss: 1.413888931274414 train acc: 45.9918\n","2 50 train loss: 1.3886022567749023 train acc: 49.5352\n","2 test loss:1.2942 test acc: 0.532\n","3 0 train loss: 1.1906986236572266 train acc: 52.0697\n","3 50 train loss: 1.1259057521820068 train acc: 54.3398\n","3 test loss:1.1543 test acc: 0.5824\n","4 0 train loss: 1.1469507217407227 train acc: 56.5861\n","4 50 train loss: 1.0890780687332153 train acc: 59.0977\n","4 test loss:1.0486 test acc: 0.6226\n","5 0 train loss: 0.9929785132408142 train acc: 61.1189\n","5 50 train loss: 1.0194835662841797 train acc: 62.1680\n","5 test loss:1.0009 test acc: 0.6398\n","6 0 train loss: 0.990096390247345 train acc: 63.0656\n","6 50 train loss: 1.0571194887161255 train acc: 64.9102\n","6 test loss:1.0650 test acc: 0.6223\n","7 0 train loss: 0.9273808002471924 train acc: 66.9016\n","7 50 train loss: 0.8536020517349243 train acc: 67.1719\n","7 test loss:0.9313 test acc: 0.6724\n","8 0 train loss: 0.8755863904953003 train acc: 68.5492\n","8 50 train loss: 0.8316375017166138 train acc: 69.5234\n","8 test loss:0.8690 test acc: 0.6899\n","9 0 train loss: 0.6976116299629211 train acc: 70.7090\n","9 50 train loss: 0.7312542200088501 train acc: 71.6328\n","9 test loss:0.8669 test acc: 0.6948\n","10 0 train loss: 0.7560838460922241 train acc: 71.6516\n","10 50 train loss: 0.8130643367767334 train acc: 73.2734\n","10 test loss:0.8181 test acc: 0.7108\n","11 0 train loss: 0.6349941492080688 train acc: 75.3607\n","11 50 train loss: 0.6680554151535034 train acc: 74.5781\n","11 test loss:0.8101 test acc: 0.7148\n","12 0 train loss: 0.6463447213172913 train acc: 75.7951\n","12 50 train loss: 0.717827558517456 train acc: 76.8750\n","12 test loss:0.7760 test acc: 0.73\n","13 0 train loss: 0.5910359025001526 train acc: 78.0820\n","13 50 train loss: 0.49932771921157837 train acc: 78.2578\n","13 test loss:0.8023 test acc: 0.72\n","14 0 train loss: 0.49593961238861084 train acc: 79.9344\n","14 50 train loss: 0.5302910804748535 train acc: 79.7305\n","14 test loss:0.7585 test acc: 0.7422\n","15 0 train loss: 0.48101961612701416 train acc: 81.5041\n","15 50 train loss: 0.48420920968055725 train acc: 81.1953\n","15 test loss:0.8423 test acc: 0.7141\n","16 0 train loss: 0.5033050775527954 train acc: 82.7623\n","16 50 train loss: 0.417883038520813 train acc: 82.8594\n","16 test loss:0.7327 test acc: 0.7538\n","17 0 train loss: 0.4419889450073242 train acc: 84.5574\n","17 50 train loss: 0.4754660427570343 train acc: 84.7500\n","17 test loss:0.7519 test acc: 0.7518\n","18 0 train loss: 0.3564484715461731 train acc: 85.9877\n","18 50 train loss: 0.3323573172092438 train acc: 85.7266\n","18 test loss:0.7870 test acc: 0.7468\n","19 0 train loss: 0.30180269479751587 train acc: 87.6926\n","19 50 train loss: 0.31715521216392517 train acc: 87.2188\n","19 test loss:0.8409 test acc: 0.749\n","20 0 train loss: 0.31127750873565674 train acc: 89.3934\n","20 50 train loss: 0.2778419554233551 train acc: 89.5156\n","20 test loss:0.8549 test acc: 0.7499\n","21 0 train loss: 0.31277701258659363 train acc: 90.6229\n","21 50 train loss: 0.2852768301963806 train acc: 89.9570\n","21 test loss:0.8571 test acc: 0.7504\n","22 0 train loss: 0.2746288776397705 train acc: 90.9180\n","22 50 train loss: 0.20088519155979156 train acc: 91.5508\n","22 test loss:0.9429 test acc: 0.7433\n","23 0 train loss: 0.24869577586650848 train acc: 91.7787\n","23 50 train loss: 0.16992664337158203 train acc: 93.5820\n","23 test loss:0.9348 test acc: 0.7508\n","24 0 train loss: 0.16450245678424835 train acc: 92.5738\n","24 50 train loss: 0.16543549299240112 train acc: 94.0508\n","24 test loss:1.0359 test acc: 0.7454\n","25 0 train loss: 0.1981276422739029 train acc: 95.3771\n","25 50 train loss: 0.09726481139659882 train acc: 95.6211\n","25 test loss:0.9460 test acc: 0.7557\n","26 0 train loss: 0.1105935201048851 train acc: 95.1352\n","26 50 train loss: 0.12878870964050293 train acc: 95.8633\n","26 test loss:1.1482 test acc: 0.746\n","27 0 train loss: 0.11733605712652206 train acc: 96.4426\n","27 50 train loss: 0.07517600804567337 train acc: 96.8516\n","27 test loss:1.1795 test acc: 0.7296\n","28 0 train loss: 0.1769677996635437 train acc: 96.3689\n","28 50 train loss: 0.0785188302397728 train acc: 96.2383\n","28 test loss:1.1496 test acc: 0.763\n","29 0 train loss: 0.07333461940288544 train acc: 97.5902\n","29 50 train loss: 0.06962369382381439 train acc: 97.7344\n","29 test loss:1.5117 test acc: 0.7171\n","30 0 train loss: 0.21658173203468323 train acc: 97.6434\n","30 50 train loss: 0.06837089359760284 train acc: 96.9883\n"]}]}]}